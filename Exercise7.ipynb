{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Exercise 7. Hyperparameter Tuning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hyperparameter Tuning\n Now that you have trained an initial model, you can tune the hyperparameters of this model to optimize model performance. Azure ML allows you to automate this tuning, in an efficient manner via early termination of poorly performing runs.\n\nYou can configure your Hyperparamter Tuning experiment by specifying the following info -\n- Define the hyparparameter space - specify ranges, distribution and sampling\n- Early Termination policy\n- Optimization metric\n- Resource / Compute budget\n- Desired concurrency\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import PyTorch\n\nscript_params = {\n    '--data_dir': ds_data.as_mount(),\n    '--num_epochs': 40,\n    '--output_dir': './outputs',\n    '--log_dir': './logs',\n    '--mode': 'fine_tune'\n}\n\nestimator10_40 = PyTorch(source_directory='.', \n                    script_params=script_params,\n                    compute_target=compute_target, \n                    entry_script='pytorch_train.py',\n                    pip_packages=['tensorboardX'],\n                    use_gpu=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import *\n\nps = RandomParameterSampling(\n    {\n        '--momentum': uniform(0.6,0.99),\n        '--learning_rate': loguniform(-6, -3)\n    }\n)\n\npolicy = BanditPolicy(evaluation_interval=2, slack_factor=0.2)\n\n\nhdc = HyperDriveRunConfig(estimator=estimator10_40, \n                          hyperparameter_sampling=ps, \n                          policy=policy, \n                          primary_metric_name='best_val_acc', \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=50,\n                          max_concurrent_runs=4)\n\nhd_run = experiment.submit(hdc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(hd_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "While the jobs is running, take a look at the [Hyperdrive documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py) to see what other options Hyperdrive offers."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# at any time, you can pull out the metrics generated so far from the runs\nhd_run.get_metrics()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}