{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exercise 4. Prepare training script\nNow you will need to create your training script. In this tutorial, the training script is already provided for you at `pytorch_train.py`. In practice, you should be able to take any custom training script as is and run it with AML without having to modify your code.\n\nHowever, if you would like to use AML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of AML code inside your training script. \n\nIn `pytorch_train.py`, we will log some metrics to our AML run. To do so, we will access the AML run object within the script:\n```Python\nfrom azureml.core.run import Run\nrun = Run.get_context()\n```\nFurther within `pytorch_train.py`, we log the learning rate and momentum parameters, the best validation accuracy the model achieves, and the number of classes in the model:\n```Python\nrun.log('lr', np.float(learning_rate))\nrun.log('momentum', np.float(momentum))\nrun.log('num_classes', num_classes)\n\nrun.log('best_val_acc', np.float(best_acc))\n```\n\nIf you downloaded the data, you can start to train the model locally (note that it will take long if you don't have a GPU -- 21 min. on a Core i7 CPU).\n\n**This step requires Pytorch to be installed locally -- find instructions [here](https://pytorch.org/#pip-install-pytorch)**\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "!mkdir outputs\n!python pytorch_train.py --data_dir breeds-10 --num_epochs 10 --output_dir outputs ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train model on the remote compute\nNow that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create an experiment\nCreate an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexperiment_name = 'pytorch-dogs' \nexperiment = Experiment(ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a PyTorch estimator\nThe AML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. The following code will define a single-node PyTorch job.\n\nThe estimator also takes a `framework_version` parameter -- if no version is provided, the estimator will default to the latest version supported by AzureML. Use `PyTorch.get_supported_versions()` to get a list of all versions supported by your current SDK version or see the [SDK documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn?view=azure-ml-py) for the versions supported in the most current release. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch)."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import PyTorch\n\nscript_params = {\n    '--data_dir': ds_data.as_mount(),\n    '--num_epochs': 10,\n    '--output_dir': './outputs',\n    '--log_dir': './logs',\n    '--mode': 'fine_tune'\n}\n\nestimator10 = PyTorch(source_directory='.', \n                      script_params=script_params,\n                      compute_target=compute_target, \n                      entry_script='pytorch_train.py',\n                      pip_packages=['tensorboardX'],\n                      use_gpu=True,\n                      framework_version='1.0',\n                      _use_framework_images=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n- We passed our training data reference `ds_data` to our script's `--data_dir` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the training data `breeds` on our datastore.\n- We specified the output directory as `./outputs`. The `outputs` directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n\nThe `_use_framework_images` parameter is in private preview. When `_use_framework_images = True`, pre-built framework images are used to run AMLCompute jobs or as intermediate images to make constructing an estimator more efficient by either eliminating or greatly reducing image build time.\n\nTo leverage the Azure VM's GPU for training, we set `use_gpu=True`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Submit job\nRun your experiment by submitting your estimator object. Note that this call is asynchronous."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "run10 = experiment.submit(estimator10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor your run\nYou can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run10).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### What happens during a run?\nIf you are running this for the first time, the compute target will need to pull the docker image, which will take about 2 minutes. This gives us the time to go over how a **Run** is executed in Azure Machine Learning. \n\nNote: had we not created the workspace with an existing ACR, we would have also had to wait for the image creation to be performed -- that takes and extra 10-20 minutes for big GPU images like this one. This is a one-time cost for a given python configuration, and subsequent runs will then be faster. We are working on ways to make this image creation faster.\n\n![](aml-run.png)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}